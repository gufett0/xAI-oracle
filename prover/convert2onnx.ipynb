{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate ONNX representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 50, 50])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "#from utils import *\n",
    "import json\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from scorecamMOD import ScoreCAM, ScoreCAMOnly\n",
    "current_dir = os.getcwd()\n",
    "project_root = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "sys.path.append(project_root)\n",
    "from training.defineSNN2 import SimpleConvNet, BiggerConvNet\n",
    "#import subprocess\n",
    "#subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"onnx\"])\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define image preprocessing\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((50, 50)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "img1_path = \"TestInvalid_t0.jpeg\"\n",
    "img2_path = \"TestInvalid_t1.jpeg\"\n",
    "img1 = Image.open(img1_path).convert('L')\n",
    "img2 = Image.open(img2_path).convert('L')\n",
    "img1 = preprocess(img1).unsqueeze(0).to(device)\n",
    "img2 = preprocess(img2).unsqueeze(0).to(device)\n",
    "print(img1.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 50, 50])\n"
     ]
    }
   ],
   "source": [
    "model_path = os.path.join('outputs/dnn/model23k.onnx')\n",
    "data_path = os.path.join('outputs/dnn/input23k.json')\n",
    "\n",
    "loaded_model = BiggerConvNet().to(device)\n",
    "#loaded_model.load_state_dict(torch.load('training/trained_NEWsnn_lr_0.1B.pth', map_location=torch.device('cpu')))\n",
    "#loaded_model.load_state_dict(torch.load('training/trained_simplesnn_lr_0.01BIG.pth', map_location=torch.device('cpu')))\n",
    "#loaded_model.load_state_dict(torch.load('training/trained_3conv_lr_0.1B.pth', map_location=torch.device('cpu')))\n",
    "#loaded_model.load_state_dict(torch.load('training/trained_SQUEEZEsnn_lr_0.1B.pth', map_location=torch.device('cpu')))\n",
    "#loaded_model.load_state_dict(torch.load('../training/trained__460__lr_0.001B.pth', map_location=torch.device('cpu')))\n",
    "loaded_model.load_state_dict(torch.load('../training/trained__23k__lr_0.001B.pth', map_location=torch.device('cpu')))\n",
    "\n",
    "loaded_model.eval()\n",
    "\n",
    "# Define image preprocessing\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((50, 50)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "img1_path = \"TestInvalid_t0.jpeg\"\n",
    "img2_path = \"TestInvalid_t1.jpeg\"\n",
    "img1 = Image.open(img1_path).convert('L')\n",
    "img2 = Image.open(img2_path).convert('L')\n",
    "img1 = preprocess(img1).unsqueeze(0).to(device)\n",
    "img2 = preprocess(img2).unsqueeze(0).to(device)\n",
    "print(img1.size())\n",
    "\n",
    "# Export the loaded Siamese Network model to ONNX\n",
    "torch.onnx.export(loaded_model,                      # loaded Siamese Network model\n",
    "                  (img1, img2),                      # model input (or a tuple for multiple inputs)\n",
    "                  model_path,                        # where to save the ONNX model\n",
    "                  export_params=True,                # store the trained parameter weights inside the model file\n",
    "                  opset_version=14,                  # the ONNX version to export the model to\n",
    "                  do_constant_folding=True,          # whether to execute constant folding for optimization\n",
    "                  input_names = ['input1', 'input2'],   # the model's input names\n",
    "                  output_names = ['output'], # the model's output names\n",
    "                  dynamic_axes={'input1' : {0 : 'batch_size'}, 'input2' : {0 : 'batch_size'},   # variable length axes\n",
    "                                'output' : {0 : 'batch_size'}})\n",
    "\n",
    "\n",
    "# Move tensors from GPU to CPU\n",
    "img1_cpu = img1.cpu()\n",
    "img2_cpu = img2.cpu()\n",
    "\n",
    "# Serialize data into a JSON file\n",
    "input_data = [\n",
    "    img1_cpu.detach().numpy().reshape([-1]).tolist(),\n",
    "    img2_cpu.detach().numpy().reshape([-1]).tolist(),\n",
    "]\n",
    "\n",
    "data = dict(input_data=input_data)\n",
    "json.dump(data, open(data_path, 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** Providing the calibrate settings function with a larger and broader range of sample inputs.\n",
    "\n",
    "cal_path = os.path.join('outputs/dnn/calib_data23k.json')\n",
    "\n",
    "path_to_dataset = '../training/dataset1k224x224'\n",
    "\n",
    "csv_path = os.path.join(path_to_dataset, 'forest_dataset.csv')\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Construct the full paths to the images\n",
    "img1_files = [os.path.join('../training/dataset1k224x224', path) for path in df['imageT0'].iloc[:20]]\n",
    "img2_files = [os.path.join('../training/dataset1k224x224', path) for path in df['imageT1'].iloc[:20]]\n",
    "\n",
    "img1_list = []\n",
    "img2_list = []\n",
    "for img1_path, img2_path in zip(img1_files, img2_files):\n",
    "    \n",
    "    img1 = Image.open(img1_path).convert('L')\n",
    "    img2 = Image.open(img2_path).convert('L')\n",
    "    img1 = preprocess(img1).unsqueeze(0).to(device)\n",
    "    img2 = preprocess(img2).unsqueeze(0).to(device)\n",
    "   #img1, img2 = siamese_loader.load_and_transform_pair(img1_path, img2_path)\n",
    "    img1 = img1.unsqueeze(0).cpu().detach().numpy().reshape([-1]).tolist()\n",
    "    img2 = img2.unsqueeze(0).cpu().detach().numpy().reshape([-1]).tolist()\n",
    "    img1_list.extend(img1) # don't use .append bc it should look like [[xxx...],[yyy...]]\n",
    "    img2_list.extend(img2)\n",
    "\n",
    "data = dict(input_data = [img1_list, img2_list])\n",
    "# Serialize data into file:\n",
    "json.dump( data, open(cal_path, 'w' ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GetScore (for aggregation circuit) conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prover.scorecamMOD import GetActivations,  GetMask, GetScore, GetCAM\n",
    "import onnx\n",
    "\n",
    "loaded_model = SimpleConvNet().to(device)\n",
    "loaded_model.load_state_dict(torch.load('../training/trained__460__lr_0.001B.pth', map_location=torch.device('cpu')))\n",
    "\n",
    "# Define image preprocessing\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((50, 50)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "img1_path = \"TestInvalid_t0.jpeg\"\n",
    "img2_path = \"TestInvalid_t1.jpeg\"\n",
    "img1 = Image.open(img1_path).convert('L')\n",
    "img2 = Image.open(img2_path).convert('L')\n",
    "img1 = preprocess(img1).unsqueeze(0).to(device)\n",
    "img2 = preprocess(img2).unsqueeze(0).to(device)\n",
    "\n",
    "#######################\n",
    "loaded_model.eval()\n",
    "\n",
    "#target_layer = loaded_model.embedding_net.features[4] \n",
    "target_layer = loaded_model.conv1\n",
    "\n",
    "input_size = (50, 50)  # Input image size\n",
    "\n",
    "activations_extractor = GetActivations(loaded_model, target_layer)\n",
    "activations = activations_extractor(img1, img2)\n",
    "\n",
    "with torch.no_grad():\n",
    "    score = loaded_model(img1, img2)\n",
    "    print(\"normal score: \", score)\n",
    "\n",
    "# resize activation maps\n",
    "def resize_activations(activations, size):\n",
    "    resized_activations = []\n",
    "    for i in range(activations.size(1)):\n",
    "        resized_activation = transforms.functional.resize(activations[:, i, :, :].unsqueeze(1), size).squeeze(1)\n",
    "        resized_activations.append(resized_activation)\n",
    "    return torch.stack(resized_activations, dim=1)\n",
    "activations = resize_activations(activations, input_size)\n",
    "\n",
    "masks_generator = GetMask(activations)\n",
    "scores_generator = GetScore(loaded_model)\n",
    "scores_generator.eval()\n",
    "\n",
    "\n",
    "# Export the scores_generator model to ONNX\n",
    "model_path = os.path.join('outputs/cam/aggr/modelGetscore460.onnx')\n",
    "torch.onnx.export(scores_generator, (img1, img2), model_path,\n",
    "                  export_params=True, opset_version=14,\n",
    "                  do_constant_folding=True, input_names=['masked_input', 'input2'],\n",
    "                  output_names=['score'], dynamic_axes={'masked_input': {0: 'batch_size'},\n",
    "                                                        'input2': {0: 'batch_size'},\n",
    "                                                        'score': {0: 'batch_size'}})\n",
    "\n",
    "# Create directory for serialized input data if it doesn't exist\n",
    "input_data_dir = os.path.join('outputs/cam/aggr/inputs')\n",
    "os.makedirs(input_data_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "all_scores = []\n",
    "for index in range(activations.size(1)):  # Loop over the number of channels\n",
    "    masked_input = masks_generator(img1, index)\n",
    "    with torch.no_grad():\n",
    "        score = scores_generator(masked_input, img2) # score_generator OR LOADED MODEL???\n",
    "        print(\"score2: \", score)\n",
    "    all_scores.append(score)\n",
    " \n",
    "    masked_input_cpu = masked_input.cpu()\n",
    "    img2_cpu = img2.cpu()\n",
    "    # INPUT SERIALIZATION\n",
    "    input_data = [\n",
    "        masked_input_cpu.detach().numpy().reshape([-1]).tolist(),\n",
    "        img2_cpu.detach().numpy().reshape([-1]).tolist(),\n",
    "    ]\n",
    "    \n",
    "    data = dict(input_data=input_data)\n",
    "    data_path = os.path.join(input_data_dir, f'input460_{index}.json')\n",
    "    with open(data_path, 'w') as f:\n",
    "        json.dump(data, f)\n",
    "\n",
    "# inspect the onnx model\n",
    "onnx_model = onnx.load(model_path)\n",
    "onnx.checker.check_model(onnx_model)\n",
    "print(\"Model loaded and checked successfully!\")\n",
    "print(onnx.helper.printable_graph(onnx_model.graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** Providing the calibrate settings function with a larger and broader range of sample inputs.\n",
    "cal_path = os.path.join('outputs/cam/aggr/calib_dataGetscore460.json')\n",
    "path_to_dataset = '../training/dataset1k224x224'\n",
    "csv_path = os.path.join(path_to_dataset, 'forest_dataset.csv')\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "\n",
    "# Construct the full paths to the images\n",
    "img1_files = [os.path.join('../training/dataset1k224x224', path) for path in df['imageT0'].iloc[:20]]\n",
    "img2_files = [os.path.join('../training/dataset1k224x224', path) for path in df['imageT1'].iloc[:20]]\n",
    "\n",
    "img1_list = []\n",
    "img2_list = []\n",
    "for img1_path, img2_path in zip(img1_files, img2_files):\n",
    "    \n",
    "    img2 = Image.open(img2_path).convert('L')\n",
    "    img2 = preprocess(img2).unsqueeze(0).to(device)\n",
    "    size= (img2.size())\n",
    "    img2 = img2.unsqueeze(0).cpu().detach().numpy().reshape([-1]).tolist()\n",
    "    \n",
    "    img1 = torch.rand(size).to(device)  # Random tensor for img1\n",
    "    img1 = img1.cpu().detach().numpy().reshape([-1]).tolist()\n",
    "    \n",
    "    img1_list.extend(img1) # don't use .append bc it should look like [[xxx...],[yyy...]]\n",
    "    img2_list.extend(img2)\n",
    "\n",
    "data = dict(input_data = [img1_list, img2_list])\n",
    "# Serialize data into file:\n",
    "json.dump( data, open(cal_path, 'w' ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GetCam conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3])\n",
      "torch.Size([1, 3, 50, 50])\n",
      "3\n",
      "7500\n",
      "Serialized inputs for GetCAM exported to outputs/cam/inputGetcam460.json\n"
     ]
    }
   ],
   "source": [
    " # EXPORT the other class GETCAM! ***\n",
    "cam_generator = GetCAM(activations.size(1))\n",
    "stacked_scores = torch.stack(all_scores, dim=1)  # Concatenate along the correct dimension\n",
    "\n",
    "print(stacked_scores.squeeze(2).size())\n",
    "print(activations.size())\n",
    "cam = cam_generator(stacked_scores, activations)    \n",
    "\n",
    "input_data_dir = os.path.join('outputs/cam')\n",
    "model_path = os.path.join('outputs/cam/modelGetcam460.onnx')\n",
    "torch.onnx.export(cam_generator, (stacked_scores.squeeze(2), activations), model_path,\n",
    "                  export_params=True, opset_version=14,\n",
    "                  do_constant_folding=True, input_names=['all_scores', 'activations'],\n",
    "                  output_names=['cam'], dynamic_axes={'all_scores': {0: 'batch_size'},\n",
    "                                                      'activations': {0: 'batch_size'},\n",
    "                                                      'cam': {0: 'batch_size'}})\n",
    "\n",
    "# Serialize inputs for ONNX model\n",
    "all_scores_cpu = stacked_scores.cpu()\n",
    "activations_cpu = activations.cpu()\n",
    "\n",
    "# Serialize input data for ONNX\n",
    "input_data = [\n",
    "    all_scores_cpu.detach().numpy().flatten().tolist(),\n",
    "    activations_cpu.detach().numpy().flatten().tolist(),\n",
    "]\n",
    "\n",
    "print(len(input_data[0]))\n",
    "print(len(input_data[1]))\n",
    "\n",
    "\n",
    "data_path = os.path.join(input_data_dir, 'inputGetcam460.json')\n",
    "data = dict(input_data=input_data)\n",
    "json.dump(data, open(data_path, 'w'))\n",
    "print(f\"Serialized inputs for GetCAM exported to {data_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[8.8819]]), tensor([[5.9736]]), tensor([[2.1965]])]\n",
      "torch.Size([1, 3, 1])\n",
      "dummy_scores_list length:  3\n",
      "dummy_scores_list length:  6\n",
      "dummy_scores_list length:  9\n",
      "dummy_scores_list length:  12\n",
      "dummy_scores_list length:  15\n",
      "dummy_scores_list length:  18\n",
      "dummy_scores_list length:  21\n",
      "dummy_scores_list length:  24\n",
      "dummy_scores_list length:  27\n",
      "dummy_scores_list length:  30\n",
      "dummy_scores_list length:  33\n",
      "dummy_scores_list length:  36\n",
      "dummy_scores_list length:  39\n",
      "dummy_scores_list length:  42\n",
      "dummy_scores_list length:  45\n",
      "dummy_scores_list length:  48\n",
      "dummy_scores_list length:  51\n",
      "dummy_scores_list length:  54\n",
      "dummy_scores_list length:  57\n",
      "dummy_scores_list length:  60\n",
      "60\n",
      "150000\n",
      "Serialized calibration data to outputs/cam/calib_dataGetcam460.json\n"
     ]
    }
   ],
   "source": [
    "# create dummy activations and outputs for Calibration\n",
    "\n",
    "cal_path = os.path.join('outputs/cam/calib_dataGetcam460.json')\n",
    "def create_dummy_data(num_samples, scores_shape, activation_shape):\n",
    "    dummy_activations_list = []\n",
    "    dummy_scores_list = []\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        # Generate random scores and activations\n",
    "        dummy_scores = torch.randn(*scores_shape).cpu().detach().numpy().tolist()\n",
    "        dummy_activations = torch.randn(*activation_shape).cpu().detach().numpy()\n",
    "\n",
    "        # Flatten nested lists and append scores\n",
    "        flattened_scores = [item for sublist in dummy_scores for inner_list in sublist for item in inner_list]\n",
    "        dummy_scores_list.extend(flattened_scores)\n",
    "\n",
    "        print(\"dummy_scores_list length: \", len(dummy_scores_list))\n",
    "\n",
    "        # Flatten activations and append\n",
    "        dummy_activations_list.extend(dummy_activations.flatten().tolist())\n",
    "\n",
    "    return dummy_scores_list, dummy_activations_list\n",
    "\n",
    "# Get the shapes of the inputs\n",
    "print(all_scores)\n",
    "stacked_scores = torch.stack(all_scores, dim=1)  \n",
    "scores_shape = stacked_scores.size()\n",
    "print(scores_shape)\n",
    "\n",
    "activation_shape = activations.size()\n",
    "\n",
    "\n",
    "# Create dummy data\n",
    "dummy_scores_list, dummy_activations_list = create_dummy_data(20, scores_shape, activation_shape)\n",
    "print(len(dummy_scores_list))\n",
    "print(len(dummy_activations_list))\n",
    "\n",
    "# Save the data to a JSON file\n",
    "data = {'input_data': [dummy_scores_list, dummy_activations_list]}\n",
    "\n",
    "\n",
    "json.dump(data, open(cal_path, 'w'))\n",
    "print(f\"Serialized calibration data to {cal_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
